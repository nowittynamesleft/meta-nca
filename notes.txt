- Training seems unstable; sometimes 500 meta-epochs gets to 96% val accuracy and sometimes it stays at 33% the whole time
- CPU training is faster (500 metaepochs with visualizations: 91 seconds with GPU, 44 seconds with CPU)
	- no visualization: 46 seconds with CPU (?), 84 seconds with GPU

- Todo:
	- Make it possible to have more than one layer. Very important. (done! although haven't gotten better accuracies with this yet)
	- What can I do to understand the way these things learn? (read stuff about learning dynamics, probably should go back to the "science of dl" slack to see if there's anything there. alternatively, could ask naomi/others from kc lab for recs)
	- How did David Ha "learn" gradient descent with local rules? find this out from paper
